{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "few-shot-learning-with-transformers",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxUyufMK/GQVt7fmSWPw2j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shea-Fyffe/transforming-personality-scales/blob/main/tutorials/few-shot-learning-with-transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ZVjSGx2hzd"
      },
      "source": [
        "# Few-Shot Learing Using Transformers (with GPT-3)\n",
        "This code is written in **Python** as an illustration of *few-shot* learning, which occurs when few labeled training examples are available (see [Ruder, 2017](https://ruder.io/transfer-learning/)). When taking a standard approach to text classification with few labeled examples, transformer architectures commonly used for text classification (e.g., *BERT*; [Devlin et al., 2019](https://arxiv.org/abs/1810.04805)) suffer inconsistent performance ([Zhang et al., 2021](http://arxiv.org/abs/2006.05987)). To overcome this researchers may choose to \"freeze\" encoder layers (e.g., [Chronopoulou et al., 2019](https://doi.org/10.18653/v1/N19-1213)); however, merely reframing the a classification task to better align with a transformer's source task seems to be a more viable option ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)).\n",
        "\n",
        "By reframing a classification task into a *language modeling* task, transformers seem to better cope with a small number of training examples (e.g., [Chronopoulou et al., 2019](https://doi.org/10.18653/v1/N19-1213); [Schick & Schütze, 2021](https://arxiv.org/abs/2009.07118)). In a language modeling task, a model is trained to predict the next word in a sequence of words; this task is somewhat universal when it comes to pretraining a transformer model, so much so that it allows large decoder models (e.g., *GPT-3*; [Brown et al., 2020](https://arxiv.org/abs/2005.14165)), which are most often used for language generation tasks, to perform text classification tasks. We demonstrate this approach by using GPT-3 to perform few-shot classification. We provide a baseline by comparing this approach to a standard approach to test classification.\n",
        "\n",
        "*Remember*: you will need to register for an API key on OpenAI's website [here](https://beta.openai.com/). There are also several open source versions available; however, they've yet to achieve GPT-3's level of performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPlCRN53ULva"
      },
      "source": [
        "### Libraries\n",
        "\n",
        "Colab comes with a large number of Python libraries pre-installed. However, `openai` and `transformers` are not libraries pre-installed libraries, however, these library can be installed by using the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7S6aRPS_w63"
      },
      "source": [
        "#@title Installing OpenAI and Transformer Libraries\n",
        "%%capture\n",
        "! pip install openai\n",
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A25eSs8QUkS8"
      },
      "source": [
        "# GPT3 related libraries\n",
        "import openai\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "# Data management libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from google.colab import drive # optional for getting data\n",
        "from typing import Dict, List # for type hinting\n",
        "\n",
        "# General utility libraries\n",
        "import os\n",
        "import sys\n",
        "import time # for sleeping between requests\n",
        "import requests # for downloading url\n",
        "from io import StringIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mounting Google Drive\n",
        "It is often a good idea to allow Colab to mount (or connect) to your Google Drive. This allows you to easily save models or—as we demonstrate—import data. By default, Colab's working directory is `/content/`, we can place our Google Drive root directory within this folder. If you've changed your current working directory, you can use `os.getcwd()` to see your current directory"
      ],
      "metadata": {
        "id": "A2ounVPbGmVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect the current working directory to a user's Google Drive account\n",
        "drive.mount(os.getcwd() + '/drive')"
      ],
      "metadata": {
        "id": "va9W9UqUGYL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d220b497-f6f1-4835-9cc8-71ba69f7f415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKw_ddT8-0Jz"
      },
      "source": [
        "## Classes and Functions\n",
        "\n",
        "Here we define a class and several class functions that will be used to train and extract classifications from an instance of `GPT-3`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FewShotGPT3:\n",
        "    \"\"\"A Few-shot learning class for the transformer GPT-3\"\"\"\n",
        "\n",
        "    def __init__(self, api_token: str, model: str = 'davinci',  multi_label: bool = False):\n",
        "        \"\"\"Initial call class  \n",
        "        Args:\n",
        "            api_key: API token from beta.openai.com\n",
        "            model: Underlying GPT-3 model to be used (e.g., ada, babbage, curie, davinci,) (optional, default: 'davinci')\n",
        "            multi_label: Conduct a multi-label prediction (optional, False)\n",
        "        \"\"\"    \n",
        "        openai.api_key = api_token\n",
        "        self.model = model\n",
        "        self.tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "        self.multi_label = multi_label\n",
        "        self.results = []\n",
        "\n",
        "    def __str__(self, verbose = False):\n",
        "        \"\"\"Custom print method \n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            return str(self.__class__) + '\\n'+ '\\n'.join(('{} = {}'.format(item, self.__dict__[item]) for item in self.__dict__))\n",
        "\n",
        "        print_res = \"GPT-3 architecture: %s\\nMulti-label classification: %s\\nTraining data imported: %s\\n\" % (self.model,\n",
        "                                                                                                              self.multi_label, \n",
        "                                                                                                              hasattr(self, \"training_data\"))\n",
        "        if hasattr(self, \"training_data\"):\n",
        "            print_res = print_res + \"Training data size: %d\\nText col: %s\\nLabel col: %s\\nUnique labes: %s\" % (len(self.training_data),\n",
        "                                                                                                              self.training_data.columns[0], \n",
        "                                                                                                              self.training_data.columns[1],\n",
        "                                                                                                              ', '.join(self.unique_labels))\n",
        "        return print_res\n",
        "\n",
        "    def format_labels(self, labels = None, ignore_case: bool = True) -> List[str]:\n",
        "        \"\"\"Format and tokenize labels \n",
        "        Args:\n",
        "            labels: A list of strings (default: None)\n",
        "            ignore_case: A boolean flag to treat labels as case-insensitive (optional, default: True)\n",
        "        \"\"\"\n",
        "        if labels is None:\n",
        "            if not hasattr(self, \"training_data\"):\n",
        "                raise AttributeError(\"Training data not yet loaded. Use `import_training_data()` before proceeding.\")\n",
        "            labels =  list(map(lambda x: x[1], self.training_data))  \n",
        "        labels = [label.strip().lower().capitalize() for label in labels]\n",
        "        self.ignore_case = ignore_case\n",
        "        self.unique_labels = list(dict.fromkeys(labels))\n",
        "        self.label_tokens = {label_value: self.tokenizer.encode(\" \" + label_value) for label_value in self.unique_labels}\n",
        "        self.label_tokens_len = [len(self.label_tokens[x]) for x in self.label_tokens if isinstance(self.label_tokens[x], list)]\n",
        "        self.label_first_tokens = {\n",
        "                    (self.tokenizer.decode([tokens[0]]).strip().lower() if self.ignore_case else tokens[0]):(label)\n",
        "                    for label, tokens in self.label_tokens.items()\n",
        "                    }\n",
        "        self.label_all_tokens = {\n",
        "                    (self.tokenizer.decode([subtoken for subtoken in tokens]).strip().lower() if self.ignore_case else tokens[0]):(label)\n",
        "                    for label, tokens in self.label_tokens.items()\n",
        "        }\n",
        "        return labels\n",
        "        \n",
        "    def subsample(self, few_shot_k: int = 1, seed: int = 42, shuffle = True):\n",
        "        \"\"\"Import a CSV of text documents with labels for few shot training\n",
        "        Args:\n",
        "            few_shot_k: Number of random examples per class to select for few shot learning (optional, default = 1)\n",
        "            seed: Random seed for sub-sampling a few examples (k) (optional, default = 42)\n",
        "            shuffle: shuffle rows in data using random seed (optional, default = True)\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"training_data\"):\n",
        "           raise AttributeError(\"Training data not yet loaded. Use `import_training_data()` before proceeding.\")\n",
        "        few_shot_data = self.training_data\n",
        "        few_shot_data = few_shot_data.groupby(few_shot_data.columns[1], group_keys=False).apply(lambda x: x.sample(n=int(few_shot_k), random_state = seed))\n",
        "        if shuffle:\n",
        "            few_shot_data = few_shot_data.sample(frac=1, random_state = seed).reset_index(drop=True)\n",
        "        self.few_shot_data = few_shot_data\n",
        "        return print('Few shot data created sucessfully')\n",
        "\n",
        "    def import_train_data(self, csv_path: str, text_col: str = \"text\", label_col: str = \"label\",  enc: str = 'latin1', shuffle = True, seed: int = 42):\n",
        "        \"\"\"Import a CSV of text documents with labels for few shot training\n",
        "        Args:\n",
        "            csv_path: A csv file path (or url)\n",
        "            text_col: Name of column in csv containing text documents\n",
        "            label_col: Name of column containing labels\n",
        "            enc: File encoding to be used (optional)\n",
        "            shuffle: shuffle rows in data (optional)\n",
        "            seed: Random seed for shuffling data (optional, default = 42)\n",
        "        \"\"\"\n",
        "        if (csv_path.startswith(\"http\")):\n",
        "            res = requests.get(csv_path,\n",
        "                               headers= {'User-Agent': 'Mozilla/5.0',\n",
        "                                         \"X-Requested-With\": \"XMLHttpRequest\"})\n",
        "            csv_path = StringIO(res.text)\n",
        "        df = pd.read_csv(csv_path, encoding = enc)\n",
        "        if shuffle:\n",
        "            df = df.sample(frac=1, random_state = seed).reset_index(drop=True)\n",
        "        df[label_col] = self.format_labels(df[label_col])\n",
        "        self.training_data = df[[text_col, label_col]]\n",
        "        return print('Data imported sucessfully')\n",
        "    \n",
        "    def import_test_data(self, csv_path: str, text_col: str = \"text\", label_col = None, enc: str = 'latin1'):\n",
        "        \"\"\"Import a CSV of text documents for prediction\n",
        "        Args:\n",
        "            csv_path: A csv file path (or url)\n",
        "            text_col: Name of column in csv containing text documents\n",
        "            label_col: Name of column containing labels (optional)\n",
        "            enc: File encoding to be used (optional)\n",
        "        \"\"\"\n",
        "        if (csv_path.startswith(\"http\")):\n",
        "            res = requests.get(csv_path,\n",
        "                               headers= {'User-Agent': 'Mozilla/5.0',\n",
        "                                         \"X-Requested-With\": \"XMLHttpRequest\"})\n",
        "            csv_path = StringIO(res.text)\n",
        "        df = pd.read_csv(csv_path, encoding = enc)\n",
        "        if label_col is not None:\n",
        "            self.test_labels = df[label_col].tolist()\n",
        "        self.test_text = df[text_col].tolist()\n",
        "        return print('Data imported sucessfully')\n",
        "\n",
        "    def predict(self, test = None, request_delay: int = 1, label_bias = 100, **kwargs):\n",
        "        \"\"\"Import a CSV of text documents for prediction\n",
        "        Args:\n",
        "            test: Documents to be predicted (optional, defaults to data imported via import_test_data)\n",
        "            request_delay: Time (in seconds) to wait between calls to API\n",
        "            label_bias: If multi-label is false biases all label tokens to be only tokens predicted (optional, default = 100)\n",
        "            kwargs: \n",
        "        \"\"\"\n",
        "        if hasattr(self, \"few_shot_data\"):\n",
        "            training_examples = self.few_shot_data\n",
        "        else:\n",
        "            training_examples = self.training_data\n",
        "        if test is None:\n",
        "            if hasattr(self, \"test_text\"):\n",
        "                test = self.test_text\n",
        "            else:\n",
        "                raise AttributeError(\"Test data not yet loaded. Use `import_test_data()` before proceeding.\")\n",
        "        args = {\n",
        "                'logprobs': len(self.unique_labels) + 1,\n",
        "                'labels': self.unique_labels\n",
        "                }\n",
        "        if not self.multi_label: \n",
        "            # This weighs only the labels specified\n",
        "            args['logit_bias'] = {\n",
        "                str(token): int(label_bias/(i + 1))\n",
        "                for tokens in self.label_tokens.values()\n",
        "                for i, token in enumerate(tokens)\n",
        "            }\n",
        "            self.label_tokens_logit_bias = args['logit_bias']\n",
        "            # When multi-label is True, GPT-3 will predict tokens equal to the longest label + 1\n",
        "            args['logprobs'] = max(self.label_tokens_len) + 1\n",
        "\n",
        "        diff_args = set(args.keys()) - set(kwargs.keys())\n",
        "\n",
        "        if diff_args:\n",
        "            args.update(kwargs)\n",
        "\n",
        "        for test_doc in test:\n",
        "            time.sleep(request_delay)\n",
        "            try:\n",
        "                self.results.append(openai.Classification.create(search_model=self.model,\n",
        "                                                        model=self.model,\n",
        "                                                        examples=training_examples.values.tolist(),\n",
        "                                                        query=test_doc,\n",
        "                                                        **args))\n",
        "            except:\n",
        "                error = sys.exc_info()[0]\n",
        "                if error == openai.error.InvalidRequestError:\n",
        "                    print(f'InvalidRequestError\\nResults received:{self.results}\\n')\n",
        "                print(\"API error:\", error)\n",
        "\n",
        "    def extract_predictions(self):\n",
        "        \"\"\"Extract predictions from GPT-3 API results\n",
        "        \"\"\" \n",
        "        first_token_to_label = self.label_first_tokens\n",
        "        prediction_log_probs = []\n",
        "        # (TODO) create multi-label method and softmax\n",
        "        results = self.results\n",
        "        for p in results:\n",
        "            top_logprobs = p[\"completion\"][\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
        "            token_probs = defaultdict(float)\n",
        "            for token, logp in top_logprobs.items():\n",
        "                if self.ignore_case:\n",
        "                    token_probs[token.strip().lower()] += np.exp(logp)\n",
        "                else:\n",
        "                    token_probs[self.tokenizer.encode(token)[0]] = np.exp(logp)\n",
        "            label_probs = {\n",
        "                first_token_to_label[token]: prob \n",
        "                for token, prob in token_probs.items()\n",
        "                if token in first_token_to_label\n",
        "            }\n",
        "            # Fill in the probability for the special \"Unknown\" label--which are predictions that weren't specified \n",
        "            if sum(label_probs.values()) < 1.0:\n",
        "                label_probs['Unknown'] = 1.0 - sum(label_probs.values())\n",
        "            prediction_log_probs.append(label_probs)\n",
        "        return prediction_log_probs\n",
        "\n",
        "    def output_predictions(self, prediction_data: List[Dict[str, float]], output_file: str = \"prediction-results.csv\"):\n",
        "        \"\"\"Output test predictions to a CSV file\n",
        "          Args:\n",
        "            prediction_data: object returned using the `extract_predictions` function \n",
        "            output_file: A a csv file path to write predictions to (optional, default = 'prediction-results.csv')\n",
        "            kwargs: \n",
        "        \"\"\"\n",
        "        out_data = pd.DataFrame(prediction_data)\n",
        "        out_data[\"predicted_label\"] = out_data.idxmax(axis=1)\n",
        "        out_data[\"predicted_label\"] = out_data[\"predicted_label\"].str.lower()\n",
        "        out_data.insert(0, \"test_text\", self.test_text)\n",
        "        if hasattr(self, \"test_labels\"):\n",
        "            out_data[\"actual_labels\"] = self.test_labels\n",
        "        out_data.to_csv(output_file, index=False)\n",
        "        return print(f\"file output to: {output_file}\")"
      ],
      "metadata": {
        "id": "WoDYi6OTBGAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7tSVUx--zg7"
      },
      "source": [
        "#@title Test Extraction and Probabilities\n",
        "def extract_prob_data(res: Dict, top_probs = True):\n",
        "    res = res[\"completion\"][\"choices\"][0][\"logprobs\"]\n",
        "    if top_probs:\n",
        "        return res[\"top_logprobs\"]\n",
        "    return res\n",
        "    \n",
        "def logprob_to_prob(logprob: float) -> float:\n",
        "    return np.exp(logprob)\n",
        "def prob_for_label(label: str, logprobs: List[Dict[str, float]]) -> float:\n",
        "    \"\"\"\n",
        "    Returns the predicted probability for the given label as\n",
        "    a number between 0.0 and 1.0.\n",
        "    \"\"\"\n",
        "    label = label.strip().lower()\n",
        "    prob = 0.0\n",
        "    next_logprobs = logprobs[0]\n",
        "    for s, logprob in next_logprobs.items():\n",
        "        s = s.strip().lower()\n",
        "        if label == s:\n",
        "            prob += logprob_to_prob(logprob)\n",
        "        elif label.startswith(s):\n",
        "            rest_of_label = label[len(s) :]\n",
        "            remaining_logprobs = logprobs[1:]\n",
        "            prob += logprob * prob_for_label(\n",
        "                rest_of_label,\n",
        "                remaining_logprobs,\n",
        "            )\n",
        "    return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Parameters"
      ],
      "metadata": {
        "id": "uhqWDPRc-0fO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPqexbkgUc6q"
      },
      "source": [
        "#@title Entering API Key\n",
        "# this can be stored as an environmental variable (ideal when using a local machine)\n",
        "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "API_KEY = \"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\"\n",
        "FEWSHOTK = 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU5mojBFURyy"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Loading GPT-3\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "First, we can load the model using our `API_KEY`. We've created a class `FewShotGPT3` that will contain everything we need for this tutorial. Additionally, we are using the specific GPT-3 version `davinci`. You can load various other GPT-3 architectures by changing the `model` argument."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Examples: Loading Other GPT-3 Architectures\n",
        "```\n",
        "# To initialized a few shot model object with 'ada' \n",
        ">>> few_shot_model = FewShotGPT3(API_KEY, model = 'ada')\n",
        "\n",
        "# To initialized a few shot model object with 'curie' \n",
        ">>> few_shot_model = FewShotGPT3(API_KEY, model = 'curie')\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "A3v_9dPaqrOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_model = FewShotGPT3(API_KEY)"
      ],
      "metadata": {
        "id": "4LuorG6oyC83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Importing and Formatting Data\n",
        "---\n",
        "There are several ways to import training data (see our [tutorial]()). Importantly, the training data should be a `csv` (or url to a csv) using the method `import_train_data` from the `few_shot_model`.\n",
        "\n",
        "By default, the `import_train_data` function assumes that the text is found in a column labeled `text` and the labels are found in the `label` column. However, this can be modified by changing the `text_col` and `label_col` arguments when calling the function."
      ],
      "metadata": {
        "id": "7UV3cbv63nDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Examples: Import Data with Various Column Names\n",
        "```\n",
        "# If your csv file (e.g., train-data.csv) contains text data in the column 'docs' and labels in the column 'labels'\n",
        ">>> few_shot_model.import_train_data(\"train-data.csv\", text_col = 'docs', label_col = 'labels')\n",
        "\n",
        "# If your csv file (e.g., my-data.csv) contains text data in the column 'text_examples' and labels in the column 'classes'\n",
        ">>> few_shot_model.import_train_data(\"my-data.csv\", text_col = 'text_examples', label_col = 'classes')\n",
        "```"
      ],
      "metadata": {
        "id": "C3N8kyMTqji_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoOpPwcYvTvc"
      },
      "source": [
        "#### Importing Training Data from online repository\n",
        "\n",
        "\n",
        "While there are several ways to import data into Colab ([see here](https://colab.research.google.com/notebooks/io.ipynb)), the most intuitive way is to use the project's code repository url:\n",
        "\n",
        "```\n",
        "# Assign the online data repository to a url so it does not have to be repeated later\n",
        "repository_data_url = \"https://anonymous.4open.science/api/repo/transforming-personality-scales/file/data/text-classification/\"\n",
        "\n",
        "few_shot_model.import_train_data(repository_data_url + \"train-data.csv\", text_col = 'docs', label_col = 'labels')\n",
        "```\n",
        "\n",
        "\n",
        "You can also upload a local `.csv` file. You can do this by:\n",
        "- Visiting the project url above and clicking the `download file` button (top right in project repository)\n",
        "- Clicking the ***Files*** pane in Colab (the folder icon on the left in Colab)\n",
        "- Clicking the ***Upload to session storage*** icon (left-most icon in Colab)\n",
        "- Selecting the local data file you would like to use (e.g., `.csv`,`.tsv`)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the online data repository to a url so it doesn't have to be repeated laterr\n",
        "repository_data_url = 'https://anonymous.4open.science/api/repo/transforming-personality-scales/file/data/text-classification/'"
      ],
      "metadata": {
        "id": "vYmz5jdG_LWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the training data\n",
        "few_shot_model.import_train_data(repository_data_url + \"train-data.csv\")"
      ],
      "metadata": {
        "id": "rIBLvU3m4R-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad43063a-76af-4d68-fb84-d5d7db4d5c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data imported sucessfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Model and Data Attributes\n",
        "The `import_train_data` stores several useful attributes, which are automatically derived using the training data. Some of the more important attributes are:\n",
        "+ **ignore_case**: A True/False flag to determine if labels should be treated as case-sensitive.\n",
        "+ **unique_labels**: The labels identified from the training data.\n",
        "+ **label_tokens**: GPT *tokenizes* words before prediction; this produces a series of index values which represent the row of each label's token(s) in GPT's pre-trained vocabulary. Label words are often tokenized into sub-word units, which can lead to complications (especially when two different labels begin with the same tokens). We recommend using short labels that are unique.\n",
        "+ **label_first_tokens**: To check the first tokens for each label, inspect the `label_first_tokens` attribute. This shows how labels were tokenized by GPT.\n",
        "+ **logit_bias_for_classification**: If performing a *multi-class* (as opposed to a *multi-label* classification a bias is added to each label first token. The reasoning for this is largely due to how GPT-3 makes predictions. When predicting the labels given sequences of text, GPT-3 reframes the task as a language modeling task (i.e., predicting the next token or word given a sequence of words). GPT has access to its *complete* vocabulary during this task. By adding a logit bias we ensure it prioritizes the tokenized labels we have provided it. However, as discussed in our Study, this mechanism can strategically be used to predict labels that are *not* provided when training (see Discussion section of manuscript)."
      ],
      "metadata": {
        "id": "30CA2v6PztfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can get an overview of the model using the print function\n",
        "print(few_shot_model) "
      ],
      "metadata": {
        "id": "iMTQa44Tzsp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7ae62f-3ab9-49ad-c720-7c43393fc984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-3 architecture: davinci\n",
            "Multi-label classification: False\n",
            "Training data imported: True\n",
            "Training data size: 733\n",
            "Text col: text\n",
            "Label col: label\n",
            "Unique labes: Neuroticism, Conscientiousness, Agreeableness, Openness, Extraversion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at your first tokens to verify they are unique\n",
        "few_shot_model.label_first_tokens"
      ],
      "metadata": {
        "id": "3STRtAnrIJE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73b3499-ba11-4959-c0d5-e43fd00978d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ag': 'Agreeableness',\n",
              " 'cons': 'Conscientiousness',\n",
              " 'extra': 'Extraversion',\n",
              " 'neuro': 'Neuroticism',\n",
              " 'open': 'Openness'}"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Testing Data\n",
        "\n",
        "Again, we can use the repository url that was specified earlier."
      ],
      "metadata": {
        "id": "iMBqdeMT5Ob6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_model.import_test_data(repository_data_url + \"test-data.csv\", label_col='label')"
      ],
      "metadata": {
        "id": "47XUX2fH5SPp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f1b781-73ed-44a7-b26c-be39ea073791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data imported sucessfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(few_shot_model.test_text)"
      ],
      "metadata": {
        "id": "S9GzxSCN5WoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab5cdb1-570f-467d-a046-6f22c25d6c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Few Shot Data by Subsampling\n",
        "Since this is an illustration of *Few-Shot* learning. We can call the `subsample`, which selects a particular number (determined by the `few_shot_k` argument) of examples *per* label. Since we have five labels—for example—setting `few_shot_k = 2` will create a few shot dataset of size 10 in our model object. "
      ],
      "metadata": {
        "id": "hGnaAkNxVqmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The subsample method will update our model object by adding a few shot dataset\n",
        "few_shot_model.subsample(FEWSHOTK)\n",
        "# You can check the newly created few shot dataset by typing in `.few_shot_data` after your model object\n",
        "few_shot_model.few_shot_data"
      ],
      "metadata": {
        "id": "nerQwcxmVlvF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "48e37b96-611a-4922-faa5-bb5f1f0de2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few shot data created sucessfully\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               text          label\n",
              "0               I enjoy being part of a loud crowd.   Extraversion\n",
              "1                I do not call people just to talk.  Agreeableness\n",
              "2                    I react strongly to criticism.  Agreeableness\n",
              "3                    I am someone who can be tense.    Neuroticism\n",
              "4    Others perceive that I am not easily bothered.    Neuroticism\n",
              "..                                              ...            ...\n",
              "195         I let things proceed at their own pace.   Extraversion\n",
              "196             I try not to think about the needy.  Agreeableness\n",
              "197                         I enjoy being reckless.   Extraversion\n",
              "198                             I dislike learning.       Openness\n",
              "199                        I bottle up my feelings.   Extraversion\n",
              "\n",
              "[200 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f0211aa-88c9-413a-9a7c-6a88b3fd157e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I enjoy being part of a loud crowd.</td>\n",
              "      <td>Extraversion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I do not call people just to talk.</td>\n",
              "      <td>Agreeableness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I react strongly to criticism.</td>\n",
              "      <td>Agreeableness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I am someone who can be tense.</td>\n",
              "      <td>Neuroticism</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Others perceive that I am not easily bothered.</td>\n",
              "      <td>Neuroticism</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>I let things proceed at their own pace.</td>\n",
              "      <td>Extraversion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>I try not to think about the needy.</td>\n",
              "      <td>Agreeableness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>I enjoy being reckless.</td>\n",
              "      <td>Extraversion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>I dislike learning.</td>\n",
              "      <td>Openness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>I bottle up my feelings.</td>\n",
              "      <td>Extraversion</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f0211aa-88c9-413a-9a7c-6a88b3fd157e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f0211aa-88c9-413a-9a7c-6a88b3fd157e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f0211aa-88c9-413a-9a7c-6a88b3fd157e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Predicting Labels of Test Cases\n",
        "---\n",
        "Since both training and testing data has been loaded into the model object, we can now classify the test cases. GPT-3's Classification API simplifies the training process by training the model and predicting test cases concurrently. One limitation to note, however, is that the Classification API may only predict one test case at a time. Thus, thr `predict()` function will loop through each test example (which can be inspected by calling `.test_text`).\n",
        "\n",
        "Additionally, with the exception of the `search_model`, `model`, `examples`, and `query` arguments, the `predict()` function allows for arguments to be passed directly to the Classification API (i.e., `openai.Classification.create()`). To see a list of additional arguments, visit the [Classification API documentation](https://beta.openai.com/docs/api-reference/classifications/create). We provide several examples below."
      ],
      "metadata": {
        "id": "IQKzs4Wz7DKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Examples: Customizing GPT-3 Classifications\n",
        "```\n",
        "# Example of increasing the temperature\n",
        "#|- Not recommended (for classification), however, this could be used in cases ...\n",
        "#|- where one would like to see possible confounding labels.\n",
        ">>> few_shot_model.predict(temperature = 0.10)\n",
        "\n",
        "# To return the default prompt used for the Classification API\n",
        ">>> few_shot_model.predict(return_prompt = True)\n",
        "\n",
        "# To limit the number of training examples the models uses for classification—for example—10:\n",
        ">>> few_shot_model.predict(max_examples = 10)\n",
        "```"
      ],
      "metadata": {
        "id": "18CvxByDqd7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Used for Prediction\n",
        "By default, the `predict()` method will use the `few_shot_data` for training the model before predicting each `test_text` case. However, if `few_shot_data` is *not* created by calling `subsample()` the model will use the complete training data. \n",
        "\n",
        "Test cases can also be specifed manually using the `test` argument:\n",
        "```\n",
        "# Instead of predicting the test data, predict manually entered text\n",
        "two_new_test_docs = ['I enjoy playing group sports.', 'When getting things done, I like to boss people around.']\n",
        ">>> few_shot_model.predict(test = two_new_test_docs)\n",
        "```\n",
        "\n",
        "Thus, this method could also be used for general text classification—keeping in mind that the API sets the number of training examples used for classification to 200. This can be modified by increasing `max_examples` when calling `predict()`. For example:\n",
        "```\n",
        "# Use up to 4000 training examples when classifying test cases\n",
        ">>> few_shot_model.predict(max_examples = 4000)\n",
        "```"
      ],
      "metadata": {
        "id": "r8HlAcpms7jA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Predictions\n",
        "We will now run the predict method."
      ],
      "metadata": {
        "id": "afMTkmw8vnlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict test cases\n",
        "few_shot_model.predict()"
      ],
      "metadata": {
        "id": "IU4qjCW1O7tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Inspecting and Outputting Predictions\n",
        "---\n",
        "Raw prediction data is stored as a list of dictionaries (one for each test case) within the model object `.results`. The `index` in the dataframe (generated by the example code block below) represents the position in the sequence that GPT-3 generated. The the token it selected is in the second column and its log probability is in the 3rd. In the `top_logprobs` column you can view the tokens GPT-3 selected among. The selected token has the lowest logprob in the `top_logprobs` cell.\n",
        "\n",
        "```\n",
        ">>> pd.DataFrame(few_shot_model.results[0][\"choices\"][0][\"logprobs\"])\n",
        "\n",
        "```\n",
        "\n",
        "We provide a simplified way to extract predictions using the `extract_predictions` function. This returns probability estimates for each label. Given we weight our label tokens prior to classification (multi-class classification), GPT-3 usually picks among the labels specified. However, in cases where tokens were generated that are different than the labels presented, we offer an \"Unknown\" label. If `multi_label` is set to `True`, then the \"Unknown\" label will represent \"everything else.\""
      ],
      "metadata": {
        "id": "tRq7gV-Z7A7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract predictions\n",
        "predicted_results = few_shot_model.extract_predictions()"
      ],
      "metadata": {
        "id": "JBR4veIMNllP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output Predictions to File\n",
        "Predictions can be output to a `csv` file using the `output_predictions()` function. Unlike many of the other functions called earlier, this function requires that we pass our predicted results to the function. One can also specify the output file name using the `output_file` argument. Here are some examples:\n",
        "\n",
        "```\n",
        "# Assigning prediction data to a new object then outputing file\n",
        ">>> predictions = few_shot_model.extract_predictions()\n",
        ">>> few_shot_model.output_predictions(predictions, \"test-preds.csv\")\n",
        "\n",
        "# A more parsimonious option\n",
        "# |- would be to call extract predictions within the function call to output_predictions\n",
        ">>> few_shot_model.output_predictions(few_shot_model.extract_predictions(), \"test-preds.csv\")\n",
        "```"
      ],
      "metadata": {
        "id": "AoFJGn4CToew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output predictions\n",
        "few_shot_model.output_predictions(predicted_results, f'few-shot-{FEWSHOTK}-results.csv')"
      ],
      "metadata": {
        "id": "HGHTpdwuaf5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4245543-243e-465b-af86-390b38601ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file output to: few-shot-40-results.csv\n"
          ]
        }
      ]
    }
  ]
}