{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "p0eCZR2ngddV"
      ],
      "authorship_tag": "ABX9TyNmJySlaShKUcWbgdtmTABp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shea-Fyffe/transforming-personality-scales/blob/main/tutorials/create-fixed-sentence-embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ZVjSGx2hzd"
      },
      "source": [
        "---\n",
        "# Creating Fixed Sentence Embeddings\n",
        "---\n",
        "This colab is written in **Python** for the creation of pre-trained *fixed* sentence embeddings using *Universal Sentence Encoder* (USE; [Cer et al., 2018](https://arxiv.org/abs/1803.11175)), *Sentence BERT*  (SBERT; [Reimers & Gurevych, 2019](https://arxiv.org/abs/1908.10084)), and mean-aggregate GloVe embeddings (GloVe; [Pennington et al., 2014](https://aclanthology.org/D14-1162/)). These examples could be extrapolated to several different types of text documents. However, we focus on generating embeddings for Big Five personality statements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPlCRN53ULva"
      },
      "source": [
        "---\n",
        "## Setup\n",
        "---\n",
        "\n",
        "### Libraries\n",
        "\n",
        "Colab comes with a large number of Python libraries pre-loaded. However, `Sentence Transformers` is not one of those libraries. The `Sentence Transformers` library can be installed by using the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7S6aRPS_w63"
      },
      "source": [
        "#@markdown __RUN:__ Installing Sentence Transformers\n",
        "\n",
        "## Uncomment command below to install Sentence Transformers\n",
        "! pip install sentence_transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A25eSs8QUkS8"
      },
      "source": [
        "#@markdown __RUN:__ Loading Libraries\n",
        "# load libraries for USE (tensorflow is a native library in Colab)\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# load sentence_tranformers for SBERT embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Util libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "from io import StringIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0eCZR2ngddV"
      },
      "source": [
        "### Using a GPU\n",
        "To speed things up you can use a *GPU* (*optional*).\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "The command below can be used to check the GPU instance, and additionally, the memory usage of the GPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get GPU status and info\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "P1HzxPtTE4pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkXTg-LmUAkH"
      },
      "source": [
        "---\n",
        "## Functions\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ekBE-p49KLC"
      },
      "source": [
        "#@markdown __RUN:__ Load user-defined function to create SBERT embeddings\n",
        "def create_sbert_embeddings(model: str, text, return_numpy = True):\n",
        "  \"\"\"Create Sentence BERT Embeddings from a list of strings\n",
        "  \n",
        "  Args:\n",
        "    model: SBERT model to import from tf hub\n",
        "    text: a list of sentences to embed\n",
        "    return_numpy: Should a numpy array be returned?\n",
        "  \"\"\"\n",
        "  embedding_model = SentenceTransformer(model)\n",
        "  sbert_embeddings = embedding_model.encode(text)\n",
        "  if return_numpy:\n",
        "    sbert_embeddings = np.array(sbert_embeddings)\n",
        "  return pd.DataFrame(sbert_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynPRIAeeVO6l"
      },
      "source": [
        "#@markdown __RUN:__ Load user-defined function to create USE embeddings\n",
        "def create_use_embeddings(model: str, text, return_numpy: bool = False):\n",
        "  \"\"\"Create Universal Sentence Embeddings from a list of strings\n",
        "  \n",
        "  Args:\n",
        "    model: USE model to import from tf hub\n",
        "    text: a list of sentences to embed\n",
        "    return_numpy: Should a numpy array be returned?\n",
        "  \"\"\"\n",
        "  embedding_model = hub.load(model)\n",
        "  use_embeddings = embedding_model(text)\n",
        "  if return_numpy:\n",
        "    use_embeddings = np.array(use_embeddings)\n",
        "  return pd.DataFrame(use_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown __RUN:__ Load user-defined function to create Aggregate Glove embeddings\n",
        "def create_agg_doc_embeddings(text, return_numpy: bool = False):\n",
        "  \"\"\"Create Aggregate Glove embeddings from list of strings\n",
        "  \n",
        "  Args:\n",
        "    text: a list of sentences to embed\n",
        "    return_numpy: Should a numpy array be returned?\n",
        "  \"\"\"\n",
        "  return create_sbert_embeddings(\"average_word_embeddings_glove.840B.300d\", text, return_numpy)"
      ],
      "metadata": {
        "id": "1MXEugXfi9ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p21aZOBc9Pvh"
      },
      "source": [
        "#@markdown __RUN:__ Load user-defined utility functions\n",
        "\n",
        "# Import Data function\n",
        "def import_data(path: str, text_col, enc = 'latin1'):\n",
        "  \"\"\"Import a CSV of sentences\n",
        "  \n",
        "  Args:\n",
        "    path: A csv file path or url pointing at CSV file\n",
        "    text_col: Name of column in csv containing sentences\n",
        "    enc: File encoding to be used (optional)\n",
        "  \"\"\"\n",
        "  if (path.startswith(\"http\")):\n",
        "      res = requests.get(path,\n",
        "                         headers= {'User-Agent': 'Mozilla/5.0',\n",
        "                                   \"X-Requested-With\": \"XMLHttpRequest\"})\n",
        "      path = StringIO(res.text)\n",
        "  df = pd.read_csv(path, encoding = enc)\n",
        "  return df[text_col].tolist(), df\n",
        "\n",
        "# Format output data function\n",
        "def format_output_data(emb_df, add_df = None, emb_names_prefix = \"f_V\"):\n",
        "  \"\"\"Format data to be output to CSV\n",
        "  \n",
        "  Args:\n",
        "    emb_df: A dictionary of embeddings DataFrames\n",
        "    add_df: A dictionary of additional DataFrames information to merge (optional)\n",
        "    emb_names_prefix: A string to prefix embedding column names (so that theyre not numbers)\n",
        "  \"\"\"\n",
        "  out_df = pd.concat(emb_df)\n",
        "  out_df = out_df.add_prefix(emb_names_prefix)\n",
        "  out_df.insert(0, \"set\", [x[0] for x in out_df.index])\n",
        "  if add_df is not None:\n",
        "      add_df = pd.concat(add_df)\n",
        "      add_df.reset_index(drop=True, inplace=True)\n",
        "      out_df.reset_index(drop=True, inplace=True)\n",
        "      out_df = pd.concat([add_df, out_df], axis=1)\n",
        "  return out_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdULdNEfUYb1"
      },
      "source": [
        "---\n",
        "## Model Selection\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPqexbkgUc6q"
      },
      "source": [
        "#@markdown __RUN:__ Define Universal Sentence Encoder's model\n",
        "use_model = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8tlBiCW5mBy"
      },
      "source": [
        "#@markdown __RUN:__ Define SBERT model\n",
        "sbert_model = \"paraphrase-mpnet-base-v2\" #@param [\"all-mpnet-base-v2\", \"paraphrase-mpnet-base-v2\", \"paraphrase-xlm-r-multilingual-v1\", \"paraphrase-distilroberta-base-v2\",\"distilbert-base-nli-stsb-quora-ranking\", \"average_word_embeddings_glove.840B.300d\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoOpPwcYvTvc"
      },
      "source": [
        "---\n",
        "## Importing and formatting Data\n",
        "---\n",
        "\n",
        "While there are several ways to import data into Colab ([see here](https://colab.research.google.com/notebooks/io.ipynb)), the most intuitive way is to use the project's code repository url:\n",
        "\n",
        "```python\n",
        "# both the training and testing data can be automatically downloaded using the repository below\n",
        "repository_data_url = 'https://github.com/Shea-Fyffe/transforming-personality-scales/tree/main/data/text-classification'\n",
        "\n",
        "# the import_data function will return a list of items and the original dataset\n",
        "train_text, train_raw_data = import_data(repository_data_url + \"train-data.csv\", \"text\")\n",
        "test_text, test_raw_data = import_data(repository_data_url + \"test-data.csv\", \"text\")\n",
        "```\n",
        "\n",
        "\n",
        "You can also upload a local `.csv` file. You can do this by:\n",
        "- Visiting the project url above and clicking the `download file` button (top right in project repository)\n",
        "- Clicking the ***Files*** pane in Colab (the folder icon on the left in Colab)\n",
        "- Clicking the ***Upload to session storage*** icon (left-most icon in Colab)\n",
        "- Selecting the local data file you would like to use (e.g., `.csv`,`.tsv`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD0E9EpLr3-V"
      },
      "source": [
        "#@markdown __RUN:__ Importing training data\n",
        "\n",
        "# Assign the online data repository to a url so it doesn't have to be repeated later\n",
        "repository_data_url = 'https://github.com/Shea-Fyffe/transforming-personality-scales/tree/main/data/text-classification'\n",
        "# the import_data function will return a list of sentences and the original dataset\n",
        "train_text, raw_train_data = import_data(repository_data_url + \"train-data.csv\", \"text\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown __RUN:__ Importing test data\n",
        "\n",
        "# the import_data function will return a list of sentences and the original dataset\n",
        "test_text, raw_test_data = import_data(repository_data_url + \"test-data.csv\", \"text\")"
      ],
      "metadata": {
        "id": "4_QODkso7kGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2KIx6yBqRL1"
      },
      "source": [
        "#@markdown __RUN:__ Inspecting Data\n",
        "\n",
        "# we can combine both text for easier use downstream\n",
        "all_text = {\"train\": train_text,\n",
        "            \"test\": test_text\n",
        "            }\n",
        "\n",
        "            \n",
        "raw_train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M28uNo1ssSF"
      },
      "source": [
        "---\n",
        "## Text Representation: Embedding Training and Testing Data\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU5mojBFURyy"
      },
      "source": [
        "---\n",
        "### Universal Sentence Encoder (USE)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### USE: Embedding Training and Testing Data\n",
        "\n",
        "Now we can move on to using the text from our actual data. We've stored as the  `train_text` and `test_text` in a dictionary called `all_text`. We can now just interate over this dictionary to encode everything."
      ],
      "metadata": {
        "id": "k3UlbMCK8hpy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8M2wx54t5-F"
      },
      "source": [
        "#@markdown __RUN:__ Create USE Embeddings\n",
        "# We use our custom function *create_use_embeddings* to produce embeddings\n",
        "# Let store them as a dictionary\n",
        "use_embeddings = {}\n",
        "for key, values in all_text.items():\n",
        "    use_embeddings[key] = create_use_embeddings(use_model, values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EpAM5LuvJTZ"
      },
      "source": [
        "#### USE: Formatting Data for Output\n",
        "We can now use the `format_output_data()` function to combine our embedding data with infromation from our raw datasets (`raw_train_data` & `raw_test_data`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cruBYieFvh06"
      },
      "source": [
        "# Remember that the first argument should be a dictionary of embedding DataFrames\n",
        "use_output_df = format_output_data(use_embeddings, {'raw_train':raw_train_data, 'raw_test':raw_test_data}, \"use_V\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxsUOiGB0IUF"
      },
      "source": [
        "#### USE: Output Data\n",
        "*Note:* if you are using Colab file will be exported to a virtual directory which can be found by using the command `%cd` (current directory) or `!pwd` (python working directory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AB-eGpM0UDG"
      },
      "source": [
        "use_output_df.to_csv(\"sentence-USE-embedding-data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm6IMLaY-lu-"
      },
      "source": [
        "---\n",
        "### Sentence BERT (SBERT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SBERT: Embedding Training and Testing Data\n",
        "\n",
        "We essentially repeat the process described above; though we must change the embedding function accordingly. This time we will use the `create_sbert_embeddings` function."
      ],
      "metadata": {
        "id": "flXdPMpFQb4J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3VcZKpWEmaG"
      },
      "source": [
        "#@markdown __RUN:__ Create SBERT Embeddings\n",
        "# We use our custom function *create_sbert_embeddings* to produce embeddings\n",
        "# Let store them as a dictionary\n",
        "sbert_embeddings = {}\n",
        "for key, values in all_text.items():\n",
        "    sbert_embeddings[key] = create_sbert_embeddings(sbert_model, values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2maBMIpKQ_Pc"
      },
      "source": [
        "#### SBERT: Formatting Data for Output\n",
        "We can now use the `format_output_data()` function to combine our embedding data with infromation from our raw datasets (`raw_train_data` & `raw_test_data`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dVRoisfQ_Pc"
      },
      "source": [
        "# Remember that the first argument should be a dictionary of embedding DataFrames\n",
        "sbert_output_df = format_output_data(sbert_embeddings, {'raw_train':raw_train_data, 'raw_test':raw_test_data}, \"sbert_V\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SBERT: Output Data\n",
        "*Note:* if you are using Colab file will be exported to a virtual directory which can be found by using the command `%cd` (current directory) or `!pwd` (python working directory)"
      ],
      "metadata": {
        "id": "JhL7oxBNRSMD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0e8u4L8FrLg"
      },
      "source": [
        "sbert_output_df.to_csv(\"sentence-SBERT-embedding-data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe-YE9_JSSDN"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### Aggregate Word Embeddings (Glove)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Glove: Embedding Training and Testing Data\n",
        "\n",
        "We essentially repeat the process described above; though we must change the embedding function accordingly. This time we will use the `create_agg_doc_embeddings` function."
      ],
      "metadata": {
        "id": "4DDmUW-eSSDN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO_NFeBUSSDN"
      },
      "source": [
        "#@markdown __RUN:__ Create GloVe Embeddings\n",
        "# We use our custom function *create_sbert_embeddings* to produce embeddings\n",
        "# Let store them as a dictionary\n",
        "glove_embeddings = {}\n",
        "for key, values in all_text.items():\n",
        "    glove_embeddings[key] = create_agg_doc_embeddings(values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jocWiS7ZSSDN"
      },
      "source": [
        "#### Glove: Formatting Data for Output\n",
        "We can now use the `format_output_data()` function to combine our embedding data with infromation from our raw datasets (`raw_train_data` & `raw_test_data`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrdGEkqOSSDN"
      },
      "source": [
        "# Remember that the first argument should be a dictionary of embedding DataFrames\n",
        "glove_output_df = format_output_data(glove_embeddings, {'raw_train':raw_train_data, 'raw_test':raw_test_data}, \"glove_V\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Glove: Output Data\n",
        "*Note:* if you are using Colab file will be exported to a virtual directory which can be found by using the command `%cd` (current directory) or `!pwd` (python working directory)"
      ],
      "metadata": {
        "id": "O93aoPEcSSDN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1no9S_gSSDN"
      },
      "source": [
        "glove_output_df.to_csv(\"aggregate-word-embedding-data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}