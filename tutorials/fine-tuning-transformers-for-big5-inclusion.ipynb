{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNeqEO3BELfePsjj2Oy46Qc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shea-Fyffe/transforming-personality-scales/blob/main/tutorials/fine-tuning-transformers-for-big5-inclusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ZVjSGx2hzd"
      },
      "source": [
        "---\n",
        "# Fine-tuning Transformer Models for Big Five Inclusion\n",
        "---\n",
        "\n",
        "This tutorial illistrates how to *fine-tune* (see [Lui et al., 2020](https://doi.org/10.1007/978-981-15-5573-2)) a binary classification **transformer** model to classify whether a scale item is related to the Big Five personality model (i.e., any one of the Big Five factors). With regard to the Big Five, there remains considerable debate about the generality of the model, and whether statistically adjacent traits could be encompassed by it (Costa & McCrae, 1995; Goldberg & Saucier, 1998; John et al., 2008) or lie wholly outside the Big Five (e.g., Ashton et al., 2004; Block, 2010; Paunonen & Jackson, 2000).\n",
        "\n",
        "To identify if related items are to the Big Five (or their focal classes of interest), we demonstrate how to train a classification model using *2* classes---Big Five items (labeled: `big5_item`) or another type of item (labeled: `other_item`).\n",
        "\n",
        "To populate the `other_item` class, we collected open-soure items from the following sources:\n",
        "+ Scales developed to measure traits beyond the Big Five (e.g., CPI, MPQ, JPI)\n",
        "+ Scales developed to measure non-personality constructs (e.g., CAT-PD, Emotional Intelligence, VIA, Vocational Interests)\n",
        "+ Synthetically generated items with incorrect grammar\n",
        "\n",
        "Here, we hope to illustrate a model that will not only flag personality items unrealted to one of the Big Five traits, but will also flag items that may be writted incorrectly or measure a non-personality construct.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPlCRN53ULva"
      },
      "source": [
        "---\n",
        "## Setup\n",
        "---\n",
        "\n",
        "Below, we provide information regarding the libraries, functions, and classes used in this tutorial. *Text Blocks* (like this) will serve as informative sign posts. `Code Blocks` which have a black background will actually perform the commands. We recommend adding a *Scratch Code Cell* (**Ctrl+Alt+N**) for running commands interactively. \n",
        "<br></br>\n",
        "**Libraries and Modules**\n",
        "\n",
        "Colab comes with a large number of Python libraries pre-loaded. However, `Transformers` is not initially available in Colab. The `Transformers` library can be installed by using the code below. More information on the `Transformers` library can be seen [here](https://huggingface.co/transformers/quicktour.html).\n",
        "<br></br>\n",
        "**User-Defined Functions and Classes**\n",
        "\n",
        "Below we provide several classes and functions to help may the process a bit easier. For each function help text is provided and can be printed via `print(fun_name.__doc__)`. For example, to see documentation for the `fine_tune()` function:\n",
        "\n",
        "```\n",
        "print(fine_tune.__doc__)\n",
        "\n",
        "Output:\n",
        "\n",
        "  Fine-tune transformer model for text classification.\n",
        "\n",
        "  A wrapper function for fine-tuning a pre-trained transformer\n",
        "  from the popular transformers library. Abstracts away many of the steps\n",
        "  involved, such as loading a tokenizer and formatting data.\n",
        "  \n",
        "  Arguments\n",
        "  ---------\n",
        "  model: a string usually returned from ``get_model()``.\n",
        "  text: a list of text.\n",
        "  labels: a list of labels.\n",
        "  train_args: a dictionary of training arguments.\n",
        "  multi_label: a boolean specifying whether perform multi-label classification (False by default).\n",
        "  max_seq_len: a string determining how to pad text sequences ('longest' by default).\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  trainer : transformers.Trainer\n",
        "    a fine-tuned transformer model.\n",
        "  tokenizer : transformers.tokenizer\n",
        "    the tokenizer of the fine-tuned model.\n",
        "\n",
        "```\n",
        "This provides the arguments that can be modified to customize the fine-tuning process.\n",
        "<br></br>\n",
        "**Using a GPU**\n",
        "\n",
        "To speed things up you can use a *GPU* (*optional*). First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "You can confirm that you have an active GPU by using the following command:\n",
        "```\n",
        "# check using a command line interface\n",
        "!nvidia-smi\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7S6aRPS_w63"
      },
      "source": [
        "#@title Install libraries\n",
        "## Uncomment command below to install Transformers\n",
        "! pip install transformers\n",
        "! pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na3GNOWWScm0"
      },
      "source": [
        "#@title Import libraries and modules\n",
        "# load relevant modules from transformers\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "# data libraries\n",
        "from torch.utils.data import Dataset # for formatting data before training\n",
        "import pandas as pd # for importing and exporting data\n",
        "\n",
        "# util libraries\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from google.colab import drive # optional for getting data\n",
        "from typing import Dict, List, Tuple # for type hinting\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import gc\n",
        "import warnings\n",
        "import requests\n",
        "from io import StringIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkXTg-LmUAkH"
      },
      "source": [
        "### Functions and Classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyQQt1yuCi2b"
      },
      "source": [
        "#@title Data-related functions\n",
        "# Custom data class\n",
        "class TextClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "# Import data function\n",
        "def import_data(path: str, text_col: str = 'text', label_col: str = 'label', enc: str = 'latin1'):\n",
        "    \"\"\"\n",
        "    Import a text data from a csv file.\n",
        "\n",
        "    A wrapper function around pandas.read_csv. Includes URL support.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    path: a string indicating a local csv file path or url.\n",
        "    text_col: a string indicating the name of column in csv containing text ('text' by default).\n",
        "    label_col: a string indicating the name of column in csv containing text labels ('label' by default).\n",
        "    enc: a string indicating csv file encoding ('latin1' by default).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[str]\n",
        "      a list of text.\n",
        "    List[str]\n",
        "      a list of labels.\n",
        "    pandas.DataFrame\n",
        "      the raw data.\n",
        "    \"\"\"\n",
        "    if (path.startswith(\"http\")):\n",
        "        res = requests.get(path,\n",
        "                           headers= {'User-Agent': 'Mozilla/5.0',\n",
        "                                     \"X-Requested-With\": \"XMLHttpRequest\"})\n",
        "        path = StringIO(res.text)\n",
        "    df = pd.read_csv(path, encoding = enc)\n",
        "    \n",
        "    if label_col is None:\n",
        "      return df[text_col].tolist(), df\n",
        "    return df[text_col].tolist(), df[label_col].tolist(), df\n",
        "\n",
        "# Format output data function\n",
        "def format_output_data(raw_outputs, test_case_ids = None, label_values = None, output_probabilities: bool = True,\n",
        "                       output_predicted_label: bool = True):\n",
        "    \"\"\"\n",
        "    Format model predictions to DataFrame.\n",
        "\n",
        "    A helper function that formats classification predictions taken from\n",
        "    ``transformers.Trainer.predict()`` into various outputs.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    raw_outputs: a numpy.ndarray of predictions from ``transformers.Trainer.predict()``.\n",
        "    test_case_ids: a list of test case ids (None by default).\n",
        "    label_values: a list of *unique ordered* labels (None by default).\n",
        "    output_probabilities: A boolean specifying whether to convert logit predictions to probabilities (True by default).\n",
        "    output_predicted_label: A boolean whether to append a 'predicted' column with the most likely label (True by default).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "      a dataset of predicted values.\n",
        "    \"\"\"\n",
        "    out_df = pd.DataFrame(raw_outputs)\n",
        "\n",
        "    if output_probabilities:\n",
        "        out_df = softmax(out_df, axis=1)\n",
        "    \n",
        "    if output_predicted_label:\n",
        "        out_df['predicted'] = np.argmax(out_df, axis=1)\n",
        "    \n",
        "    if label_values is not None:\n",
        "        out_df.columns = label_values\n",
        "    \n",
        "    if test_case_ids is not None:\n",
        "        out_df.insert(0, 'id', test_case_ids)\n",
        "    return out_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p21aZOBc9Pvh"
      },
      "source": [
        "#@title Model-related functions\n",
        "# Custom fine-tuning function\n",
        "def fine_tune(model, text, labels, train_args, multi_label: bool = False, max_seq_len: str = 'longest', **kwargs):\n",
        "    \"\"\"\n",
        "    Fine-tune transformer model for text classification.\n",
        "  \n",
        "    A wrapper function for fine-tuning a pre-trained transformer\n",
        "    from the popular transformers library. Abstracts away many of the steps\n",
        "    involved, such as loading a tokenizer and formatting data.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    model: a string usually returned from ``get_model()``.\n",
        "    text: a list of text.\n",
        "    labels: a list of labels.\n",
        "    train_args: a dictionary of training arguments.\n",
        "    multi_label: a boolean specifying whether perform multi-label classification (False by default).\n",
        "    max_seq_len: a string determining how to pad text sequences ('longest' by default).\n",
        "    kwargs: additional keyword arguments to pass to ``Trainer.__init__``.\n",
        "  \n",
        "    Returns\n",
        "    -------\n",
        "    trainer : transformers.Trainer\n",
        "      a fine-tuned transformer model.\n",
        "    tokenizer : transformers.tokenizer\n",
        "      the tokenizer of the fine-tuned model.\n",
        "    \"\"\"\n",
        "    _, model_name = get_model(model)\n",
        "  \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  \n",
        "    train_labels_indx, lab_to_id, num_labs = map_labels_to_keys(labels)\n",
        "    \n",
        "    if max_seq_len == 'longest':\n",
        "      train_encodings = tokenizer(text, truncation=True, padding=True)\n",
        "    else:\n",
        "      train_encodings = tokenizer(text, padding='max_len', max_length=max_seq_len)\n",
        "  \n",
        "    train_dataset = TextClassificationDataset(train_encodings, train_labels_indx)\n",
        "      \n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name, num_labels=num_labs, label2id = lab_to_id\n",
        "        )\n",
        "    # setting multi_label = True with less than two labels will default to a binary classification\n",
        "    if multi_label and num_labs > 2:\n",
        "      model.problem_type = \"multi_label_classification\"\n",
        "  \n",
        "    trainer = Trainer(model=model,\n",
        "        args = training_args,\n",
        "        train_dataset = train_dataset,\n",
        "        **kwargs\n",
        "      )\n",
        "   \n",
        "    trainer.train()\n",
        "    trainer.tokenizer = tokenizer\n",
        "    return trainer\n",
        "\n",
        "# Get model for simple transformers\n",
        "def get_model(model_type: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Get pre-trained transformer model.\n",
        "    \n",
        "    A helper function that looks up pre-trained model given model type. If model\n",
        "    is *not* found in lookup will ouput string used as input.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    model_type: a string indicating model type name (e.g., 'bart', 'bert', 'deberta', 'xlnet').\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    model_name : tuple(str, str)\n",
        "      a tuple of the model type and specific model name.\n",
        "      \n",
        "    See Also\n",
        "    --------\n",
        "    See https://huggingface.co/models for the complete repository of usable transformer models      \n",
        "    \"\"\"\n",
        "    model_dict = {\n",
        "    'albert': (\"albert\", \"albert-xlarge-v2\"),\n",
        "    'bart': (\"bart\", \"facebook/bart-large\"),\n",
        "    'bert': (\"bert\", \"bert-base-cased\"),\n",
        "    'deberta': (\"debertav2\", \"microsoft/deberta-v3-large\"),\n",
        "    'distilbert': (\"distilbert\", \"distilbert-base-cased-distilled-squad\"),\n",
        "    'distilroberta': (\"roberta\", \"cross-encoder/stsb-distilroberta-base\"),\n",
        "    'electra': (\"electra\", \"cross-encoder/ms-marco-electra-base\"),\n",
        "    'roberta': (\"roberta\", \"roberta-large\"),\n",
        "    'xlnet': (\"xlnet\", \"xlnet-large-cased\"),\n",
        "    'xmlroberta': (\"xmlroberta\", \"xlm-roberta-large\"),\n",
        "    }\n",
        "    # if model is not found will try model_type as model_name\n",
        "    model_name = model_dict.get(model_type, (model_type, model_type))\n",
        "    # returns a Tuple  \n",
        "    return model_name\n",
        "  \n",
        "# Compute evaluation metrics\n",
        "def flag_items(items: List[str], trained_model, prediction_threshold = .50, anchor_label: str = \"big5_item\", **kwargs):\n",
        "    \"\"\"\n",
        "    Flag items based on some threshold.\n",
        "\n",
        "    A helper function that flags items based on a classification probability threshold. \n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    items: list of items to flag.\n",
        "    trained_model: a trained transformer model of class Trainer.\n",
        "    prediction_threshold: a decimal number between 0.00 and 1.00 to flag items.\n",
        "    kwargs: additional keyword arguments to pass to ``transformers.Trainer.predict()``.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "      a dataset of label probabilities and a flag.\n",
        "    \"\"\"\n",
        "    items_encodings = trained_model.tokenizer(items, truncation=True, padding=True)\n",
        "    items_dataset = TextClassificationDataset(items_encodings)\n",
        "    \n",
        "    label_vals = [*get_labels(trained_model).keys()]\n",
        "    anchor_indx = label_vals.index(anchor_label)\n",
        "    \n",
        "    predictions, _, _ = trained_model.predict(items_dataset, **kwargs)\n",
        "    \n",
        "    out_df = pd.DataFrame(predictions)\n",
        "    out_df.columns = label_vals\n",
        "    \n",
        "    out_df = softmax(out_df, axis = 1)\n",
        "    \n",
        "    out_df[\"flag\"] = out_df.iloc[:, anchor_indx] < prediction_threshold\n",
        "    \n",
        "    out_df.insert(0, 'item', items)\n",
        "    return out_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTOCF_TI7LoI"
      },
      "source": [
        "#@title Utility functions\n",
        "# Map labels to keys\n",
        "def map_labels_to_keys(labels: List[str], sort_labels: bool = True):\n",
        "    \"\"\"\n",
        "    Map text labels to integers.\n",
        "    \n",
        "    This function maps a list of strings to integers.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    labels: a list of labels\n",
        "    sort_labels: a boolean specifying if labels should be sorted alphabetically before recoding (True by default)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[str]\n",
        "      a list of labels.\n",
        "    dict{str : int}\n",
        "      a dictionary where labels are keys and mapped int are values.\n",
        "    int\n",
        "      the number of class labels.\n",
        "    \"\"\"\n",
        "    k = list(dict.fromkeys(labels))\n",
        "    if sort_labels:\n",
        "      k.sort()\n",
        "    labels_to_id = {k[i] : int(i) for i in range(0, len(k))}\n",
        "    labels_out = []\n",
        "    for j in labels:\n",
        "      labels_out.append(labels_to_id[j])\n",
        "    return labels_out, labels_to_id, len(k)\n",
        "\n",
        "# Helper to return labels from trained model\n",
        "def get_labels(trained_model):\n",
        "    \"\"\"\n",
        "    Return list of class labels from a model returned by `Trainer.train()`\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    trained_model: a trained transformer model of class Trainer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[str]\n",
        "      a list of labels.\n",
        "    \"\"\"\n",
        "    return trained_model.model.config.label2id\n",
        "     \n",
        "# Helper to check for GPU device and garbage collect\n",
        "def get_gpu():\n",
        "    \"\"\"\n",
        "    Check if CUDA compatible GPU is available.\n",
        "\n",
        "    To manually check if you are able to use a GPU environment in Colab click\n",
        "    the `Runtime` menu above, then select `Change Runtime Type`, the pick \"GPU\"\n",
        "    for the `Hardware Accelerator` dropdown.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "      number of current CUDA GPU device. If -1, no was found. \n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "      return torch.cuda.current_device()\n",
        "    else:\n",
        "      return -1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdULdNEfUYb1"
      },
      "source": [
        "---\n",
        "## Selecting Model and Hyper-Parameters\n",
        "---\n",
        "We define our variables for purposes described in our research manuscript. However, we encourage researchers and practitioners to try out alternative models (by manually overriding `transformer_model`). In addition, we wanted to minimize the tuning hyper-parameters during training as the aim of this research is to highlight Transformers in a baseline sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8tlBiCW5mBy"
      },
      "source": [
        "#@title Select pre-trained transformer model\n",
        "transformer_model = \"deberta\" #@param [\"deberta\", \"albert\", \"bert\", \"bart\", \"distilbert\",\"distilroberta\", \"electra\", \"roberta\", \"xlnet\", \"xlmroberta\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GAQl22KulEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02532f07-21b8-4bb3-e242-bccac8d8bd8f"
      },
      "source": [
        "#@title Define training hyper-parameters\n",
        "\n",
        "# length to pad items to (~each word is 1.15 sequence units)\n",
        "SEQ_LEN = 32\n",
        "\n",
        "# first we can initialized the ClassificationArguments object\n",
        "training_args = TrainingArguments(\n",
        "   num_train_epochs = 10,\n",
        "   learning_rate = 2e-5,\n",
        "   warmup_ratio = 0.10,\n",
        "   weight_decay = 0.01,\n",
        "   per_device_train_batch_size = 32,\n",
        "   seed = 42,\n",
        "   evaluation_strategy=\"no\",\n",
        "   save_strategy=\"no\",\n",
        "   logging_strategy=\"epoch\",\n",
        "   output_dir = f\"{transformer_model}/outputs\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU5mojBFURyy"
      },
      "source": [
        "---\n",
        "## Uploading and Importing Data\n",
        "---\n",
        "\n",
        "**Uploading Data**\n",
        "\n",
        "While there are several ways to import data into Colab ([see here](https://colab.research.google.com/notebooks/io.ipynb)), the most intuitive way is to use the project's code repository url:\n",
        "\n",
        "```\n",
        "# Assign the online data repository to a url so it does not have to be repeated later\n",
        "repository_data_path = \"https://anonymous.4open.science/api/repo/transforming-personality-scales/file/raw-data/\"\n",
        "```\n",
        "\n",
        "As an alternative, you can also upload a local `.csv` file. You can do this by:\n",
        "- Visiting the project url above and clicking the `download file` button (top right in project repository)\n",
        "- Clicking the ***Files*** pane in Colab (the folder icon on the left in Colab)\n",
        "- Clicking the ***Upload to session storage*** icon (left-most icon in Colab)\n",
        "- Selecting the local data file you would like to use (e.g., `.csv`,`.tsv`)\n",
        "\n",
        "If using this method, the path to the file can be used. To locate the file path using the *Colab File Pane* (folder icon on the left-hand side). Generally, uploaded files will be in the `/content/` directory. Once the file is found, right click the file and select \"Copy path.\" This path can be pasted into the `import_data` function directly or assigned to an object that can be used later.\n",
        "\n",
        "```\n",
        "local_file_path = \"content/supplemental-item-data.csv\"\n",
        "```\n",
        "</br>\n",
        "\n",
        "**Importing Data**\n",
        "\n",
        "To properly import the training data we must specify the file path, column name containing our items, and column name containing our labels. Then, the `import_data()` returns three objects:\n",
        "\n",
        "- a list (vector) of items\n",
        "- a list (vector) of labels\n",
        "- a copy of our training data\n",
        "\n",
        "```\n",
        "# Example using the url\n",
        "train_text, train_labels, train_raw_data = import_data(repository_data_path + \"supplemental-item-data.csv\", \"text\", \"label\")\n",
        "\n",
        "# Example using a local file path\n",
        "train_text, train_labels, train_raw_data = import_data(\"/\" + local_file_path, \"text\", \"label\")\n",
        "```\n",
        "\n",
        "The code above assigns these to objects names `train_text`, `train_labels` and `raw_data` respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M28uNo1ssSF"
      },
      "source": [
        "#### Importing the training\n",
        "\n",
        "We will now import the training data named---`supplemental-item-data.csv`. These data can be found on our [GitHub repo](https://anonymous.4open.science/r/transforming-personality-scales/raw-data/supplemental-item-data.csv) in the directory `raw-data/`.\n",
        "\n",
        "Unlink our primary example, we will use all of the data collected to train the model. The model can be evaluated during training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the online data repository to a url so it doesn't have to be repeated laterr\n",
        "repository_data_url = 'https://anonymous.4open.science/api/repo/transforming-personality-scales/file/raw-data/'\n",
        "\n",
        "# the import_data function will return a list of sentences, a list of labels, and the original dataset\n",
        "train_text, train_labels, raw_training_data = import_data(repository_data_url + 'supplemental-item-data.csv', \"text\", \"label\")"
      ],
      "metadata": {
        "id": "vYmz5jdG_LWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we show the first 20 items in the training set\n",
        "# ... and their corresponding labels\n",
        "for x,y in zip(train_text[:20], train_labels[:20]):\n",
        "    print(\"Item: %s | Label: %s\" %(x, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HznvjAPZD_P",
        "outputId": "491f0f28-8912-43c2-e88f-a9944a8feae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item: Others perceive that I act before thinking. | Label: big5_item\n",
            "Item: Being in debt is worrisome to me. | Label: other_item\n",
            "Item: Others perceive that I believe in the importance of art. | Label: big5_item\n",
            "Item: Others perceive that I believe people have bad intentions. | Label: big5_item\n",
            "Item: Others perceive that I cannot handle a lot of information. | Label: big5_item\n",
            "Item: Others perceive that I my control desires. | Label: big5_item\n",
            "Item: Others perceive that I disregard rules to get ahead. | Label: big5_item\n",
            "Item: Others perceive that I do not have original ideas and I am not inventive. | Label: big5_item\n",
            "Item: Others perceive that I do not joke around. | Label: big5_item\n",
            "Item: Others perceive that I do not stop to contemplate my life. | Label: big5_item\n",
            "Item: Others perceive that I enjoy being with people. | Label: big5_item\n",
            "Item: Others perceive that I get irritated easily. | Label: big5_item\n",
            "Item: Good manners and cleanliness matter to me. | Label: other_item\n",
            "Item: Good manners are very important. | Label: other_item\n",
            "Item: Others perceive that I hang around doing nothing. | Label: big5_item\n",
            "Item: Others perceive that I have frequent mood changes. | Label: big5_item\n",
            "Item: I abuse people confidences. | Label: other_item\n",
            "Item: I accept apologies easily. | Label: big5_item\n",
            "Item: I accept challenging tasks. | Label: big5_item\n",
            "Item: I accept little from others. | Label: other_item\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm6IMLaY-lu-"
      },
      "source": [
        "---\n",
        "## Training the Model\n",
        "---\n",
        "\n",
        "To clarify: *fine-tuning* is a specific type of training applied to models that have been pre-trained. This process allows the model to update its parameters to better align with our classification task.\n",
        "\n",
        "The `fine-tune()` function requires that we define four arguments. We provide a description of each and the (`object`) holding such data:\n",
        "- The model or type of transformer model (`transformer_model`)\n",
        "- Text or personality items (`train_text`)\n",
        "- Text or item class labels (`train_labels`)\n",
        "- The training hyper parameters (`trainings_args`) \n",
        "\n",
        "This results in a function call that looks like:\n",
        "\n",
        "```\n",
        "fine_tuned_model, tokenizer = fine_tune(model = transformer_model,\n",
        "                                        text = train_text,\n",
        "                                        labels = train_labels,\n",
        "                                        train_args = training_args)\n",
        "```\n",
        "\n",
        "There are several *optional* arguments, such as `max_seq_len` which determines how long text is truncated (discussed below). Additionally, there's the `multi_label` argument&mdash;by setting `multi_label` to `True` i.e., `fine_tune(..., multi_label = True)` one can train a model that will treat items as multi-dimensional, so items may belong to multiple classes at once.\n",
        "\n",
        "\n",
        "**Tokenizing**\n",
        "\n",
        "The `fine_tune()` function outputs the fine-tuned model (i.e., `fine_tuned_model`) and add modelâ€™s tokenizer to the object (i.e., `fine_tuned_model.tokenizer`). This step ensures both the testing and training items will be tokenized in the same way. Since we will not input the test data to the `fine_tune` function, the model's tokenizer object (i.e., `fine_tuned_model.tokenizer`) will be used right before flagging new items (within the `flag_items()` function).\n",
        "\n",
        "The `fine_tuned_model.tokenizer()` function has several notable arguments&mdash;`truncation` and `padding`. While truncation is not relevant to our case (because personality items tend to be relatively short text documents), setting `truncation=True` ensures that any document longer than the specified sequence length is truncated. Setting `padding=True`, ensures that any document shorter than the specified sequence length is padded up to that point. Usually transformers default sequence length to 512 tokens; however, it is best practice to set it to a number that is roughly 150% of the words in the longest text document.\n",
        "\n",
        "**Evaluating the Model**\n",
        "\n",
        "Since, we are using *all* of our training data, the model will not output evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_tmG_6v-3oC"
      },
      "source": [
        "# tune the model using the labeled personality items\n",
        "fine_tuned_model = fine_tune(transformer_model, train_text, train_labels, training_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldtRKgWMtGIP"
      },
      "source": [
        "---\n",
        "## Flagging New Items\n",
        "---\n",
        "\n",
        "Since we've trained the model, we can now use the `flag_items()` function to predict if new personality items as well as other types of text documents (e.g., survey responses, social media comments, and performance evaluations) are related to the Big Five or not.\n",
        "\n",
        "This function takes in new items in addition to a trained model, then outputs prediction probabilities and a `flag` variable representing if the item was *below* the `prediction_threshold`. The `prediction_threshold` is .50 by default.\n",
        "\n",
        "```\n",
        "# Create some example items\n",
        "example_flag_items =  [\"I enjoy ice cream\", \"Swimming is an interesting hobby\", \"I am the life of the party\"]\n",
        "\n",
        "# flag items example items based on a .50 threshold\n",
        "flagged_items_1 = flag_items(example_flag_items, fine_tuned_model)\n",
        "\n",
        "flagged_items_1\n",
        "\n",
        "# flag items example items based on a .90 threshold\n",
        "flagged_items_2 = flag_items(example_flag_items, fine_tuned_model, prediction_threshold = .90)\n",
        "\n",
        "flagged_items_2\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FTP0LlVsHol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "178b93fb-34d9-4ff7-e61d-98b0996c4742"
      },
      "source": [
        "#@title Flag New Items\n",
        "test_items = [\"I enjoy ice cream\", \"Swimming is an interesting hobby\", \"I make up stories about things that happened that are totally untrue.\",\n",
        "              \"I am the life of the party\", \"Life of party the I am\", \"Others consider me more of a leader than a follower\", \n",
        "              \"I consider myself more of a leader than a follower\", \"The capital of canada is ottawa\", \"zzzz\", \"I know that the capital of France is Paris\"]\n",
        "\n",
        "# Run the function using the default threshold of .50\n",
        "flagged_items = flag_items(test_items, fine_tuned_model)\n",
        "\n",
        "# Print results\n",
        "flagged_items"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 10\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                item  big5_item  other_item  \\\n",
              "0                                  I enjoy ice cream   0.004494    0.995506   \n",
              "1                   Swimming is an interesting hobby   0.000132    0.999868   \n",
              "2  I make up stories about things that happened t...   0.001991    0.998009   \n",
              "3                         I am the life of the party   0.999744    0.000255   \n",
              "4                             Life of party the I am   0.001961    0.998039   \n",
              "5  Others consider me more of a leader than a fol...   0.999536    0.000464   \n",
              "6  I consider myself more of a leader than a foll...   0.997658    0.002342   \n",
              "7                    The capital of canada is ottawa   0.000803    0.999197   \n",
              "8                                               zzzz   0.001063    0.998937   \n",
              "9         I know that the capital of France is Paris   0.000419    0.999581   \n",
              "\n",
              "    flag  \n",
              "0   True  \n",
              "1   True  \n",
              "2   True  \n",
              "3  False  \n",
              "4   True  \n",
              "5  False  \n",
              "6  False  \n",
              "7   True  \n",
              "8   True  \n",
              "9   True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-671e5391-e212-4d8a-81bd-16072a960714\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item</th>\n",
              "      <th>big5_item</th>\n",
              "      <th>other_item</th>\n",
              "      <th>flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I enjoy ice cream</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.995506</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Swimming is an interesting hobby</td>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.999868</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I make up stories about things that happened t...</td>\n",
              "      <td>0.001991</td>\n",
              "      <td>0.998009</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I am the life of the party</td>\n",
              "      <td>0.999744</td>\n",
              "      <td>0.000255</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Life of party the I am</td>\n",
              "      <td>0.001961</td>\n",
              "      <td>0.998039</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Others consider me more of a leader than a fol...</td>\n",
              "      <td>0.999536</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I consider myself more of a leader than a foll...</td>\n",
              "      <td>0.997658</td>\n",
              "      <td>0.002342</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The capital of canada is ottawa</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.999197</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>zzzz</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.998937</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>I know that the capital of France is Paris</td>\n",
              "      <td>0.000419</td>\n",
              "      <td>0.999581</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-671e5391-e212-4d8a-81bd-16072a960714')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-671e5391-e212-4d8a-81bd-16072a960714 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-671e5391-e212-4d8a-81bd-16072a960714');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWAyf4Rw1TGg"
      },
      "source": [
        "---\n",
        "### Saving the Model\n",
        "---\n",
        "This model can be saved and used to flag non-Big Five items before being sent to the model trained in our [primary example](https://colab.research.google.com/drive/1dNMJ2BuRu2l3JZq1TH0B2Fp6_WEoThXB?usp=sharing).\n",
        "\n",
        "```\n",
        "# Save the fine tuned model\n",
        "fine_tuned_model.save_model(\"fine-tuned-big5-inclusion-model\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIdA0Ibs1fOB"
      },
      "source": [
        "#@title Save fine-tuned model\n",
        "# Uncomment the line below to save the fine-tuned model for later use\n",
        "# fine_tuned_model.save_model(\"fine-tuned-big5-inclusion-model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CgVMp7OL7Pw"
      },
      "source": [
        "---\n",
        "### Flagging New Examples\n",
        "---\n",
        "\n",
        "```\n",
        "# Load Python libraries\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "# Import model to flag new items\n",
        "big5_inc_model = AutoModel.from_pretrained(\"fine-tuned-big5-inclusion-model\")\n",
        "big5_inc_tokenizer = AutoTokenizer.from_pretrained(\"fine-tuned-big5-inclusion-model\")\n",
        "\n",
        "# Create classification pipeline\n",
        "flag_items = pipeline(\"text-classification\", model=big5_inc_model, tokenizer=big5_inc_tokenizer)\n",
        "\n",
        "# Import or generate items to flag (taken from openpsychometrics.org)\n",
        "new_items = [\"I put family first.\",  \n",
        "             \"When other people are arguing, I leave the room.\", \n",
        "             \"I have a bland facial expression when I talk to people.\", \n",
        "             \"Does your heart ever thump in your ears so that you cannot sleep?\"]\n",
        "\n",
        "# Flag Items\n",
        "results = flag_items(new_items)\n",
        "```\n"
      ]
    }
  ]
}